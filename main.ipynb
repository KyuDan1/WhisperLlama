{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyudan/anaconda3/envs/py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "whisper = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-large-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "llama = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수정된 모델의 총 블록 수: 4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "def modify_llama_blocks(model, num_blocks_to_keep=2):\n",
    "    \"\"\"\n",
    "    Llama 모델의 시작과 끝 부분에서 각각 지정된 수의 블록만 남기고 중간 블록들을 제거합니다.\n",
    "    \n",
    "    Args:\n",
    "        model: Llama 모델\n",
    "        num_blocks_to_keep: 시작과 끝에서 각각 유지할 블록의 수\n",
    "    \n",
    "    Returns:\n",
    "        수정된 모델\n",
    "    \"\"\"\n",
    "    # 모델 복사\n",
    "    modified_model = deepcopy(model)\n",
    "    \n",
    "    # decoder layers 가져오기\n",
    "    layers = modified_model.model.layers\n",
    "    \n",
    "    # 전체 블록 수\n",
    "    total_blocks = len(layers)\n",
    "    \n",
    "    # 유지할 블록의 인덱스\n",
    "    keep_indices = list(range(num_blocks_to_keep)) + list(range(total_blocks - num_blocks_to_keep, total_blocks))\n",
    "    \n",
    "    # 새로운 블록 리스트 생성\n",
    "    new_layers = torch.nn.ModuleList([layers[i] for i in keep_indices])\n",
    "    \n",
    "    # 기존 layers를 새로운 것으로 교체\n",
    "    modified_model.model.layers = new_layers\n",
    "    \n",
    "    return modified_model\n",
    "\n",
    "# 사용 예시\n",
    "modified_llama = modify_llama_blocks(llama, num_blocks_to_keep=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size:  {'input_ids': tensor([[128000,  58368, 105622,  21028, 105605, 107497,  16969]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[128000,  58368, 105622,  21028, 105605, 107497,  16969]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "입력: 오늘의 날씨는\n",
      "출력: 오늘의 날씨는 “” ”’’jk ‘ '”.uhl),\n",
      "\n",
      " [...]\n",
      "\n",
      "\n",
      "\n",
      "tr you ### \\( $$﻿ _(\n",
      "\n",
      " ``` [_...\n",
      "\n",
      " ^{}'''\">\n",
      " `_”’ ’u ^{[ \n",
      "\n",
      "﻿\n",
      "mart[^._\n",
      "\n",
      ">\n",
      " '\\use....\n",
      "\n",
      " Katz.'''\n",
      " жеятьiiihe '' ”blo \"\"\n",
      "\n",
      " \"\"''ii‬се')]h _(eedeedeemej'^\"...limeii'))').'d)”’me \"_’s‘ ’bloodmodedsdevdevdevb?[millionmillionmillion billion billion\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 토크나이저 로드 및 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # EOS 토큰을 패딩 토큰으로 사용\n",
    "tokenizer.padding_side = \"left\"  # 왼쪽에 패딩 추가\n",
    "\n",
    "def generate_text(model, text, max_length=100):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에 대해 Llama 모델을 사용하여 텍스트를 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        model: Llama 모델\n",
    "        text: 입력 텍스트\n",
    "        max_length: 생성할 최대 토큰 수\n",
    "    \n",
    "    Returns:\n",
    "        생성된 텍스트\n",
    "    \"\"\"\n",
    "    # 입력 텍스트를 토큰화\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    print(\"input size: \", inputs)\n",
    "    # GPU가 있다면 GPU로 이동\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    print(inputs)\n",
    "    # 모델을 평가 모드로 설정\n",
    "    model.eval()\n",
    "    \n",
    "    # 텍스트 생성\n",
    "    with torch.no_grad():\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.2,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,  # 시작 토큰 ID 추가\n",
    "        )\n",
    "    \n",
    "    # 생성된 텍스트 디코딩\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# 모델 사용 전 패딩 토큰 설정\n",
    "modified_llama.config.pad_token_id = tokenizer.pad_token_id\n",
    "modified_llama.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 사용 예시\n",
    "input_text = \"오늘의 날씨는\"\n",
    "generated_text = generate_text(modified_llama, input_text)\n",
    "print(f\"입력: {input_text}\")\n",
    "print(f\"출력: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WhisperLlamaASR(nn.Module):\n",
    "    def __init__(self, whisper_encoder, llama_decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = whisper_encoder\n",
    "        self.decoder = llama_decoder\n",
    "        \n",
    "        self.encoder_dim = 1280\n",
    "        self.decoder_dim = 2048\n",
    "        \n",
    "        self.bridge = nn.Sequential(\n",
    "            nn.Linear(self.encoder_dim, self.decoder_dim),\n",
    "            nn.LayerNorm(self.decoder_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # 모든 컴포넌트를 같은 디바이스로 이동\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, input_features, decoder_input_ids, decoder_attention_mask=None):\n",
    "        # 입력을 모델과 같은 디바이스로 이동\n",
    "        device = next(self.parameters()).device\n",
    "        input_features = input_features.to(device)\n",
    "        decoder_input_ids = decoder_input_ids.to(device)\n",
    "        if decoder_attention_mask is not None:\n",
    "            decoder_attention_mask = decoder_attention_mask.to(device)\n",
    "            \n",
    "        # Whisper encoder를 통과\n",
    "        encoder_outputs = self.encoder(\n",
    "            input_features=input_features,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "        bridge_hidden_states = self.bridge(encoder_hidden_states)\n",
    "        \n",
    "        # Llama decoder에 전달\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            # encoder_hidden_states=bridge_hidden_states,  # cross-attention이 없으므로 제거\n",
    "            use_cache=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        return decoder_outputs\n",
    "\n",
    "def create_asr_model(whisper_encoder, llama_decoder):\n",
    "    # GPU가 있으면 GPU로 이동\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    whisper_encoder = whisper_encoder.to(device)\n",
    "    llama_decoder = llama_decoder.to(device)\n",
    "    \n",
    "    model = WhisperLlamaASR(whisper_encoder, llama_decoder)\n",
    "    return model.to(device)\n",
    "\n",
    "def process_batch(model, input_features, decoder_input_ids, decoder_attention_mask=None):\n",
    "    # 모델의 디바이스 확인\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # 입력 데이터를 모델과 같은 디바이스로 이동\n",
    "    input_features = input_features.to(device)\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "    if decoder_attention_mask is not None:\n",
    "        decoder_attention_mask = decoder_attention_mask.to(device)\n",
    "    \n",
    "    outputs = model(\n",
    "        input_features=input_features,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        decoder_attention_mask=decoder_attention_mask\n",
    "    )\n",
    "    return outputs\n",
    "\n",
    "# 사용 예시\n",
    "whisper = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-large-v2\")\n",
    "whisper_encoder = whisper.model.encoder\n",
    "asr_model = create_asr_model(whisper_encoder, modified_llama)\n",
    "\n",
    "# 예시 입력 데이터 생성 (CPU에서 생성)\n",
    "input_features = torch.randn(1, 80, 3000)\n",
    "decoder_input_ids = torch.randint(0, 32000, (1, 100))\n",
    "\n",
    "# 모델 실행 (process_batch 내에서 GPU로 이동)\n",
    "outputs = process_batch(asr_model, input_features, decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디코딩된 텍스트:  of\\\\\\\\ mir _Trat atroySw Securitiesas Outof _of ThB \"_\"kyhEcr.svg \" \" Seymouryas sPR\" \" acido SE\" mir\"es fuel Tower 'k'’ 'k \"”u ‘”” \" \" \" \"\" ”” \" \"””\"”, \" ”...\n",
      "\n",
      "deskoub...\n",
      "\n",
      "'d \" “ audiences  \"jose uvelu \" #####kj’ sek “ \"  \"neenc kes “\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def decode_asr_output(outputs, tokenizer, skip_special_tokens=True):\n",
    "    \"\"\"\n",
    "    ASR 모델의 출력을 텍스트로 변환합니다.\n",
    "    \n",
    "    Args:\n",
    "        outputs: 모델의 출력 (CausalLMOutputWithPast)\n",
    "        tokenizer: Llama 토크나이저\n",
    "        skip_special_tokens: 특수 토큰 스킵 여부\n",
    "    \n",
    "    Returns:\n",
    "        str: 디코딩된 텍스트\n",
    "    \"\"\"\n",
    "    # logits에서 가장 높은 확률을 가진 토큰 인덱스 선택\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # 토큰을 텍스트로 디코딩\n",
    "    decoded_text = tokenizer.decode(predictions[0], skip_special_tokens=skip_special_tokens)\n",
    "    \n",
    "    return decoded_text\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 직접 디코딩\n",
    "text = decode_asr_output(outputs, tokenizer)\n",
    "print(\"디코딩된 텍스트:\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_asr_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 229\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Eval Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 196\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# 모델 준비\u001b[39;00m\n\u001b[1;32m    195\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_asr_model\u001b[49m(whisper_encoder, modified_llama)\n\u001b[1;32m    197\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# 옵티마이저 설정\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_asr_model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "class LibriSpeechDataset(Dataset):\n",
    "    def __init__(self, split=\"train.clean.100\", processor=None):\n",
    "        self.dataset = load_dataset(\"librispeech_asr\", split=split)\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 오디오 데이터 로드\n",
    "        audio = self.dataset[idx][\"audio\"]\n",
    "        text = self.dataset[idx][\"text\"]\n",
    "        \n",
    "        # Whisper 프로세서로 오디오 특성 추출\n",
    "        input_features = self.processor(\n",
    "            audio[\"array\"],\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.squeeze(0)\n",
    "        \n",
    "        # 텍스트 토큰화\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"decoder_input_ids\": labels[\"input_ids\"].squeeze(0),\n",
    "            \"decoder_attention_mask\": labels[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels[\"input_ids\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 배치 내의 최대 길이에 맞춰 패딩\n",
    "    max_audio_len = max(x[\"input_features\"].shape[1] for x in batch)\n",
    "    max_text_len = max(x[\"decoder_input_ids\"].shape[0] for x in batch)\n",
    "    \n",
    "    input_features = []\n",
    "    decoder_input_ids = []\n",
    "    decoder_attention_mask = []\n",
    "    labels = []\n",
    "    \n",
    "    for item in batch:\n",
    "        # 오디오 특성 패딩\n",
    "        padded_audio = torch.nn.functional.pad(\n",
    "            item[\"input_features\"],\n",
    "            (0, max_audio_len - item[\"input_features\"].shape[1])\n",
    "        )\n",
    "        input_features.append(padded_audio)\n",
    "        \n",
    "        # 텍스트 패딩\n",
    "        padded_text = torch.nn.functional.pad(\n",
    "            item[\"decoder_input_ids\"],\n",
    "            (0, max_text_len - item[\"decoder_input_ids\"].shape[0]),\n",
    "            value=processor.tokenizer.pad_token_id\n",
    "        )\n",
    "        decoder_input_ids.append(padded_text)\n",
    "        \n",
    "        # 어텐션 마스크 패딩\n",
    "        padded_mask = torch.nn.functional.pad(\n",
    "            item[\"decoder_attention_mask\"],\n",
    "            (0, max_text_len - item[\"decoder_attention_mask\"].shape[0])\n",
    "        )\n",
    "        decoder_attention_mask.append(padded_mask)\n",
    "        \n",
    "        # 라벨 패딩\n",
    "        padded_labels = torch.nn.functional.pad(\n",
    "            item[\"labels\"],\n",
    "            (0, max_text_len - item[\"labels\"].shape[0]),\n",
    "            value=-100  # loss 계산 시 무시될 패딩 값\n",
    "        )\n",
    "        labels.append(padded_labels)\n",
    "    \n",
    "    return {\n",
    "        \"input_features\": torch.stack(input_features),\n",
    "        \"decoder_input_ids\": torch.stack(decoder_input_ids),\n",
    "        \"decoder_attention_mask\": torch.stack(decoder_attention_mask),\n",
    "        \"labels\": torch.stack(labels)\n",
    "    }\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 데이터를 디바이스로 이동\n",
    "        input_features = batch[\"input_features\"].to(device)\n",
    "        decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
    "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # 모델 forward pass\n",
    "        outputs = model(\n",
    "            input_features=input_features,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask\n",
    "        )\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = F.cross_entropy(\n",
    "            outputs.logits.view(-1, outputs.logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        \n",
    "        # Backward pass 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # WandB에 로그 기록\n",
    "        wandb.log({\n",
    "            \"batch_loss\": loss.item(),\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, eval_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader, desc='Evaluating'):\n",
    "            input_features = batch[\"input_features\"].to(device)\n",
    "            decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
    "            decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_features=input_features,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                decoder_attention_mask=decoder_attention_mask\n",
    "            )\n",
    "            \n",
    "            loss = F.cross_entropy(\n",
    "                outputs.logits.view(-1, outputs.logits.size(-1)),\n",
    "                labels.view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(eval_loader)\n",
    "    return avg_loss\n",
    "\n",
    "def main():\n",
    "    # WandB 초기화\n",
    "    wandb.init(project=\"whisper-llama-asr\")\n",
    "    \n",
    "    # 모델과 프로세서 준비\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "    \n",
    "    \"\"\"# 데이터셋 및 데이터로더 준비\n",
    "    train_dataset = LibriSpeechDataset(split=\"train.clean.10\", processor=processor)\n",
    "    eval_dataset = LibriSpeechDataset(split=\"validation\", processor=processor)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    eval_loader = DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4\n",
    "    )\"\"\"\n",
    "    \n",
    "    # 모델 준비\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = create_asr_model(whisper_encoder, modified_llama)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 옵티마이저 설정\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # 학습 설정\n",
    "    num_epochs = 10\n",
    "    best_eval_loss = float('inf')\n",
    "    \n",
    "    # 학습 루프\n",
    "    for epoch in range(num_epochs):\n",
    "        # 훈련\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "        \n",
    "        # 평가\n",
    "        eval_loss = evaluate(model, eval_loader, device)\n",
    "        \n",
    "        # WandB에 로그 기록\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"eval_loss\": eval_loss,\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "        \n",
    "        # 모델 저장\n",
    "        if eval_loss < best_eval_loss:\n",
    "            best_eval_loss = eval_loss\n",
    "            torch.save(model.state_dict(), f'best_model_epoch_{epoch}.pt')\n",
    "        \n",
    "        print(f'Epoch {epoch}: Train Loss = {train_loss:.4f}, Eval Loss = {eval_loss:.4f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
